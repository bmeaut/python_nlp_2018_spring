{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to NLP tasks\n",
    "\n",
    "The main goal of Natural Language Processing is to convert free text to / from a formal representation that allows us to process the linguistic data algorithmically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are two directions:\n",
    "* **Analysis**: text $\\rightarrow$ formal representation\n",
    "* **Generation**: formal representation $\\rightarrow$ text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The linguistic pipeline\n",
    "\n",
    "The tasks we are going to cover:\n",
    "* **Tokenization**\n",
    "* Morphological analysis / **POS tagging**\n",
    "* Syntactic parsing\n",
    "* Semantic parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "These tasks are usually incorporated in a **linguistic pipeline**:\n",
    "\n",
    "![Pipeline](pipeline.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sequentiality\n",
    "\n",
    "Natural language is inherently sequential:\n",
    "* Words are sequences of characters\n",
    "* Sentences are sequences of words\n",
    "* Paragraphs are sequences of sentences\n",
    "* Documents are sequences of paragraphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As such, all the tasks above are implemented with algorithms that work on *sequences*:\n",
    "* Dynamic programming (algorithmic)\n",
    "* Sequence classification (machine learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NLP Software\n",
    "\n",
    "#### [Stanford CoreNLP](http://nlp.stanford.edu:8080/corenlp/)\n",
    "\n",
    "* Written in Java; can be used via the API, GUI, command line or a web service.\n",
    "* Supports several major languages (other than English): Chinese, Spanish, Arabic, French and German."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### [Spacy](https://spacy.io/)\n",
    "\n",
    "* Written in Python; very simple API\n",
    "* Supports 27+ languages (to some extent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### [e-magyar](http://e-magyar.hu/hu/)\n",
    "\n",
    "* A pipeline that includes most state-of-the-art NLP tools written for Hungarian\n",
    "* Java + XML; most components are very similar to CoreNLP's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [Tokenization](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html)\n",
    "\n",
    "The computer does not know anything about words or any human language. We have to tell it what the (basic) units are.\n",
    "\n",
    "<!-- From the data point of view, written language is a sequence of characters. To introduce one of the earliest and most fundamental task in NLP, we consider the text as the list of words. Breaking the text into words is not always that easy as in the example, that is called __tokenization__. We deal with tokenized texts for now. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Tokenization\n",
    "\n",
    "Split the string into **tokens**:\n",
    "```Python\n",
    "In[1]: tokenize(\"Mr. and Mrs. Dursley of Number Four, Privet Drive, were proud to say that they were perfectly normal, thank you very much.\")\n",
    "Out[1]: ['Mr.', 'and', 'Mrs.', 'Dursley', 'of', 'Number', 'Four', ',', 'Privet', 'Drive', ',', 'were', 'proud', 'to', 'say','that', 'they', 'were', 'perfectly', 'normal', ',', 'thank', 'you', 'very', 'much', '.']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A **token** can be\n",
    "* a word\n",
    "* a punctuation mark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`str.split()` is not enough:\n",
    "* `\"much.\"` $\\rightarrow$ `\"much\" + \".\"`\n",
    "* `\"Mr.\"` $\\rightarrow$ `\"Mr.\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Sentence splitting\n",
    "\n",
    "Similarly: split the tokenized text into sentences.\n",
    "\n",
    "```Python\n",
    "In[1]: ssplit(['Me', 'Tarzan', '.', 'You', 'Jane', '.'])\n",
    "Out[2]: [['Me', 'Tarzan', '.'],\n",
    "         ['You', 'Jane', '.']]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Conversion to IDs\n",
    "\n",
    "To the computer, words are just abstract labels. Usually we put the words into a vocabulary and assign an integer ID to all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cat': 3, 'The': 4, 'on': 0, 'sat': 5, 'the': 2, 'mat': 1}\n",
      "------------------\n",
      "['The', 'cat', 'sat', 'on', 'the', 'mat']\n",
      "[4, 3, 5, 0, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "vocabulary = {w:i for i, w in enumerate(set(x.split()))}\n",
    "print(vocabulary)\n",
    "print(\"------------------\")\n",
    "print(words)\n",
    "print([vocabulary[w] for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For now, this is what we have to work with, but later you will learn more about representing words (other than meaningless IDs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## (POS) Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Part-of-speech (POS)\n",
    "\n",
    "Words can be put into categories according to their grammatical properties, or the role they can play in a sentence.\n",
    "The category into which a particular word falls is its **part-of-speech**.\n",
    "\n",
    "<!--A __Part-of-speech__ tag is a property of words. More precisely, we put words into categories according what role they play in a given sentence.-->\n",
    "\n",
    "#### Example\n",
    "\n",
    "| The|dog|saw| a|cat|.|\n",
    "|----|---|---|--|---|---|\n",
    "| determiner|noun|verb| determiner|noun|punctuation|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<!-- The categorization is aimed to label interchangeable words with the same label. For example __cat__ could be replaced with __dog__, __boat__ or any other nouns, but not with a verb.\n",
    "-->\n",
    "Interchangeable words should end up in the same category.\n",
    "  * One can change \"_saw_\" to \"_smelled_\" or \"_cat_\" to \"_mouse_\" without loss of grammaticality, but not e.g. \"_cat_\" to \"_smelled_\".\n",
    "  * On the other hand, \"_saw_\" could also be changed to \"_walked_\": POS categories do not take semantics into consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The correct POS depends on the context, not only the word itself.\n",
    "\n",
    "<!--|You|talk|the|talk|but|do|you|walk|the|walk|?|\n",
    "|---|----|---|----|---|--|---|----|---|----|-|\n",
    "|pronoun|verb|determiner|noun|conjunction|verb|pronoun|verb|determiner|noun|punctuation|-->\n",
    "\n",
    "|You|talk|the|talk|but|do|\n",
    "|---|----|---|----|---|--|\n",
    "|pronoun|verb|determiner|noun|conjunction|verb|\n",
    "|**you**|**walk**|**the**|**walk**|**?**|\n",
    "|pronoun|verb|determiner|noun|punctuation|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### POS tagging\n",
    "... is a task to label every word in a given text with the appropriate POS **tags**. An algorithm for that will be our very first NLP task!\n",
    "\n",
    "### Tagsets\n",
    "* The tags themselves are the result of linguistic/NLP consensus\n",
    "  * there are several conventions for them. \n",
    "* From computational point of view, there is no definition for POS tags\n",
    "  * Benchmark datasets are the definition (gold standard) of what the correct tags are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "From [Universal Dependencies](https://github.com/UniversalDependencies/UD_English) using [Universal POS tags](http://universaldependencies.org/u/pos/all.html):\n",
    "\n",
    "|This|killing|of|a|respected|cleric|will|be|\n",
    "|---|----|---|---|---|----|---|---|\n",
    "|DET|NOUN|ADP|DET|ADJ|NOUN|AUX|AUX|\n",
    "|**causing**|**us**|**trouble**|**for**|**years**|**to**|**come**|**.**|\n",
    "|VERB|PRON|NOUN|ADP|NOUN|PART|VERB|PUNCT|\n",
    "\n",
    "\n",
    "The Universal tagset is language independent, except some language specific features. For example the words _\"cleric\"_ and _\"gazdaság\"_ are both NOUNs. In English  _\"a\"_ and _\"the\"_ are determiners, in Hungarian _\"a\"_ and _\"az\"_ have similar grammatical functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Or a [Hungarian one](https://github.com/UniversalDependencies/UD_Hungarian)\n",
    "\n",
    "|A|gazdaság|ilyen|mértékű|fejlődését|több|folyamat|gerjeszti|.|\n",
    "|-|--------|-----|-------|----------|----|--------|---------|-|\n",
    "|DET|NOUN|DET|ADJ|NOUN|DET|NOUN|VERB|PUNCT|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "From [UMBC webbase](http://ebiquity.umbc.edu/resource/html/id/351) corpus using [Penn Treebank tagset](https://ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html):\n",
    "\n",
    "|Well| ,| let| me| just| say| there| is| n't| too|\n",
    "|---|--|---|----|---|----|---|----|---|---|\n",
    "|RB |, |VB |PRP |RB |VBP |EX |VBZ |RB |RB |\n",
    "|**much**|**I**| **can**| **tell**| **you**| **right**| **at**| **the**| **moment**| **.**|\n",
    "|JJ |PRP |MD |VB |PRP |RB |IN |DT |NN |.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The latter is just for English, note the EX (existential _there_) tag and the token _\"n't\"_ after _\"is\"_ with tag RB.\n",
    "\n",
    "These are also tokenized!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tagging in general\n",
    "<!-- In general, a tagging task requires a labeled dataset: a list of symbols with a list of corresponding target symbols.\n",
    "\n",
    "The \"sentence\" is a list of source symbols (tokens or words), and the target symbols are from a (more-or-less) well defined set.\n",
    "-->\n",
    "\n",
    "|$word_1$| $word_2$| $word_3$| $\\ldots$|\n",
    "|---|--|---|----|\n",
    "|$tag_1$ |$tag_2$  |$tag_3$  |$\\ldots$ |\n",
    "\n",
    "> __Definition__ (Token)\n",
    "> _The atoms of interest, in our case the words of a text._\n",
    "\n",
    "> __Definition__ (Corpus)\n",
    "> _A list of tokens_\n",
    "\n",
    "> __Definition__ (Tagset)\n",
    "> _A finite (usually small) set of symbols, which are linguistically defined properties of tokens._\n",
    "\n",
    "> __Definition__ (Labeled corpus)\n",
    "> _A List of (token, tag) pairs, where the tags are from a given tagset._\n",
    "\n",
    "> __Definition__ (Tagging)\n",
    "> _Take a given (unlabeled) corpus and tagset. The_ tagging _is the task of assigning tags to tokens._\n",
    "\n",
    "Sometimes a corpus is split at sentence boundaries, which means that sentences can be processed separately. \n",
    "Otherwise, a sentence boundary is just a special punctuation or end-of-sentence symbol.\n",
    "\n",
    "There are cases when the tag of a word cannot be deduced within a sentence, only in context of other sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Approaches\n",
    "\n",
    "If the tags were algorithmically well-defined, then implementing that definition would result in a 100% correct tagger without any further ado. Needless to mention, this is not the case.\n",
    "\n",
    "There are two main approaches to tagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Rule based\n",
    "Based on manually created rules.\n",
    "* written by linguists; requires great effort; expensive\n",
    "* high precision, low recall\n",
    "* example: English Constraint Grammar (from [Apertium](https://github.com/apertium/apertium-eng/blob/master/apertium-eng.eng.rlx)).\n",
    "```\n",
    "#Lemmas that are N or V: Inf if preposition precedes it\n",
    "SELECT Inf IF (0 Inf) (0 N) (-1 To) ;\n",
    "SELECT Inf IF (0 Inf) (0 N) (-1 Adv) (-2 To) ;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### __Statistical__\n",
    "\n",
    "Based on machine learning.\n",
    "* the models are trained on annotated gold standard corpora\n",
    "    * Penn Treebank (PTB, for English)\n",
    "    * Szeged Corpus (for Hungarian)\n",
    "* \"_statistical_\" because the model is conditioned on the training corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Terminology:\n",
    "<!-- , we split the data into two parts. One for training, this is at the disposal of our algorithm.\n",
    "The other part of the split is for testing. The correct labels are stripped off of the test set and are compared to the output of the algorithm.-->\n",
    "\n",
    "> __Annotating__\n",
    "> _The human labor of making the gold dataset. Manually processing sentences and label them correctly._\n",
    "\n",
    "> __Gold data__\n",
    "> _Annotated corpus, usually annotated with serious efforts and for a specific task._\n",
    "\n",
    "> __Silver data__\n",
    "> _Not that good quality or automatically labeled data._\n",
    "\n",
    "<!-- Sometimes we call the correctly labeled dataset __gold data__, usually annotated with serious efforts. If a given data is not perfectly labeled, or come from unknown origin, or the labels themselves are questionable, them we can talk about __silver data__. Silver is not always correct, but without any better at hand, they are used as training data. Sometimes silver data is acquired with automated or semi-automated techniques rather than human annotation.\n",
    "\n",
    "It is worth mentioning that one can evaluate a model on the training data, but it does not tell much about the correctness of the algorithm. If you train an algorithm and use the training set in the prediction, thats called __test on train__.  Every sound algorithm is expected to perform well (if not perfectly) on training data, since that data was available during the design/making of the algorithm. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluation\n",
    "\n",
    "Let's suppose that one has a tagger and performed the tagging on the test set.\n",
    "\n",
    "|token|$w_1$|$w_2$|$\\ldots$|\n",
    "|:--|-----|-----|--------|\n",
    "|gold labels|$l_1$|$l_2$|$\\ldots$|\n",
    "|predicted labels|$p_1$|$p_2$|$\\ldots$|\n",
    "\n",
    "* The predicted and gold labels are compared to each other.\n",
    "* The performance of a tagger can be measured several ways:\n",
    "  * per-token accuracy:\n",
    "$$\\frac{\\# \\text{ correct labels}}{\\# \\text{ words}}$$\n",
    "  * per-sentence accuracy:\n",
    "$$\\frac{\\# \\text{ sentences with all correct labels}}{\\# \\text{ sentences}}$$\n",
    "  * unknown word accuracy:\n",
    "$$\\frac{\\# \\text{ correct labels of OOV words}}{\\# \\text{ OOV words}}$$\n",
    "\n",
    "OOV is out-of-vocabulary, words that were seen in test time, but not in training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other tagging tasks\n",
    "\n",
    "Beside POS tagging, there are several other tasks.\n",
    "\n",
    "#### NER\n",
    "Named entity recognition.\n",
    "\n",
    "> <u>Uber</u> isn't pulling out of <u>Quebec</u> just yet.<br>\n",
    "> <u>Bill Gates</u> buys <u>Apple</u>. <u>DJIA</u> crashing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Mark names of entities in text\n",
    "  * people\n",
    "  * places\n",
    "  * organizations\n",
    "* A very important task in _information extraction_. Helps identify what real word entities play role in a given sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the above example the target labels are just $\\{0,1\\}$, i.e. marking whether it is a named entity or not. There are more detailed NER tagsets which tells you what that entity is (plus one tag for _\"not a named entity\"_).\n",
    "\n",
    "|tagset|# of tags| language independent | |\n",
    "|:---|:--|:--:|:--|\n",
    "|CoNLL| 4  |yes|Person, Location, Organization, Misc|\n",
    "|MUC-7| 7 | yes|also Date, Time, Numbers, Percent|\n",
    "|Penn Treebank|22|no|Animal, Cardinal, Date, Disease,...|\n",
    "\n",
    "It is a different task to match various forms of the same entity, like: _USA_, _United States of America_, _The States_, _'merica_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### NP-chunking\n",
    "\n",
    "| He | reckons |the current account deficit| will narrow| to |only £1.8 billion |in|September|.|\n",
    "|:--:|:-------:|:-------------------------:|:----------:|:--:|:----------------:|:-:|:------:|:-:|\n",
    "|NP  |         |NP                         |            |    |    NP            |   |      NP|  |\n",
    "\n",
    "The task is to find __noun phrases__ that:\n",
    "* refer to (not necessarily named) entities or things\n",
    "* correspond to grammatical roles (subject, object...) in the sentence\n",
    "\n",
    "It is often called __shallow parsing__ because it finds some syntactic components of the sentence, but not the whole structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive methods\n",
    "Here we discuss some naive approaches and common mistakes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### The tag (label) of a word is not a function of the word itself.\n",
    "* It depends on the context, the surrounding words.\n",
    "* Most of the words can have several part-of-speech tags.\n",
    "* In English noun-verbs are common: _work_, _talk_, _walk_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Named entities are not always proper nouns\n",
    "Neither start with capital letters.\n",
    "\n",
    "Counterexamples:\n",
    "* the States\n",
    "* von Neumann\n",
    "\n",
    "Sentences start with capital letters, irrespective of whether the first word is a named entity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### There is no comprehensive list of all named entities\n",
    "* Let's suppose that one wants to collect every famous person, geographic place, company, trademark and title (real and fictional ever) in a list.\n",
    "* That list will never be comprehensive, since a well known thing can be mentioned in an unusual or abbreviated form.\n",
    "* And the list would became obsolete very soon, since new movies, books, famous persons and firms are born.\n",
    "* Still, lists provide a good starting point for statistical models and the creation of silver standard corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Challenges\n",
    "\n",
    "In this section we discuss some of the main challenges / difficulties of tagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Training data\n",
    "\n",
    "* Bad quality or insufficient training data.\n",
    "* Luckily, the aforementioned tasks have well established gold standards. Still,\n",
    "    * every dataset has mistakes in it\n",
    "    * gold standard corpora are usually very small\n",
    "        * English: PTB (1M) $\\ll$ UMBC Webbase (3B)\n",
    "        * Hungarian: Szeged (1.5M) $\\ll$ Webcorpus (500M) $<$ MNSZ (1B)\n",
    "\n",
    "There is a trade-off between the quality and quantity. Human annotated data are of higher quality but lower in quantity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Evaluation\n",
    "\n",
    "* Without proper evaluation, there is no way of knowing how good the model is.\n",
    "* Given a certain amount of gold data, you have to decide how to split it into train and testing.\n",
    "* If you have no test data, you can examine the output of your algorithm yourself, but\n",
    "    * lacks statistical perspective\n",
    "    * getting 10 of your favorite sentences right doesn't mean that your algorithm is any good!\n",
    "    * this is called __testing by glance__ and you are advised to avoid it.\n",
    "<!-- Without any test data, you can compete your algorithm against humans, or see it for yourself. This kind of manual test is expensive in human time and lacks statistical perspective if you don't use enough annotators. Getting 10 of your favorite sentences right doesn't mean that your algorithm rocks!\n",
    "-->\n",
    "\n",
    "This is a serious problem in machine translation, where you don't even have a correct automated testing.\n",
    "<!-- A sentence can have several equally good translations, so if your algorithm does not give you the desired result, that doesn't mean that the translation was wrong. It's difficult to even compare sentences and tell that they mean a similar thing, if they have a different wording. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Linguistic changes\n",
    "\n",
    "* Healthy languages change over time:\n",
    "    * new words and expressions are introduced every day\n",
    "    * the grammar also changes, but much slower\n",
    "* New linguistic theories are also proposed\n",
    "    * No (or very small) gold standard corpora for the less popular ones\n",
    "    * Conversion of gold standards\n",
    "        * Not always possible\n",
    "        * Usually the quality is lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hidden Markov Model (HMM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A simple POS tagger\n",
    "\n",
    "* Let's write a very simple tagger.\n",
    "* Given the corpus, establish\n",
    "    * a vocabulary $V$ with every word in it\n",
    "    * and the set of labels $L$\n",
    "* The labeled corpus is a list of pairs in $V\\times L$.\n",
    "* Usually $|V|\\approx 10^5, 10^6$ and $|L|$ is never more than $100$\n",
    "\n",
    "The model: a lookup table which assigns a POS tag to a word based on the list of $n$ previous words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corpus:\n",
      "[('The', 'DET'), ('quick', 'ADJ'), ('brown', 'ADJ'), ('fox', 'NOUN'), ('jumps', 'VERB'), ('over', 'ADP'), ('the', 'DET'), ('lazy', 'ADJ'), ('dog', 'NOUN'), ('.', 'PUNCT')]\n",
      "\n",
      "The lookup table:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([(('The',), 'DET'),\n",
       "             (('The', 'quick'), 'ADJ'),\n",
       "             (('The', 'quick', 'brown'), 'ADJ'),\n",
       "             (('quick', 'brown', 'fox'), 'NOUN'),\n",
       "             (('brown', 'fox', 'jumps'), 'VERB'),\n",
       "             (('fox', 'jumps', 'over'), 'ADP'),\n",
       "             (('jumps', 'over', 'the'), 'DET'),\n",
       "             (('over', 'the', 'lazy'), 'ADJ'),\n",
       "             (('the', 'lazy', 'dog'), 'NOUN'),\n",
       "             (('lazy', 'dog', '.'), 'PUNCT')])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "# Two inputs: the corpus and n\n",
    "corpus=list(zip([\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\", \".\"],\n",
    "                [\"DET\", \"ADJ\", \"ADJ\", \"NOUN\", \"VERB\", \"ADP\", \"DET\", \"ADJ\", \"NOUN\", \"PUNCT\"]))\n",
    "n = 3\n",
    "\n",
    "print('The corpus:', corpus, sep='\\n')\n",
    "pos_lookup = OrderedDict([(tuple(corpus[j][0] for j in range(max(i - 2, 0), i + 1)),\n",
    "                           corpus[i][1])\n",
    "                          for i in range(len(corpus))])\n",
    "print('\\nThe lookup table:')\n",
    "pos_lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Markov model\n",
    "\n",
    "The keys in the table are called **n-grams**.\n",
    "\n",
    "The lookup table is a **Markov model**:\n",
    "* _Model_: given a word and $n-1$ previous words, the POS tag can be looked up (if the n-gram is in the corpus)\n",
    "* _Markov assumption_: the POS tag depends only on the last $n$ words\n",
    "\n",
    "Context can help:\n",
    "> work $\\rightarrow$ it can be noun or verb<br>\n",
    "> my <u>work</u> $\\rightarrow$ noun<br>\n",
    "> I <u>work</u> $\\rightarrow$ verb<br>\n",
    "\n",
    "The longer the context, the better the prediction $-$ and the higher the memory requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Other models\n",
    "\n",
    "Clearly, the Markov assumption does not hold for natural language, which has _long distance dependencies_. More powerful models exist:\n",
    "\n",
    "* You can use the words after the target word\n",
    "  * but not the gold tags, not even before the target word.\n",
    "* Temporarily, you can use previously predicted labels as if they were correct\n",
    "  * but in the end, test prediction cannot use any gold labels.\n",
    "\n",
    "The algorithm is as follows:\n",
    "* look up a given word with the surrounding words.\n",
    "* If the sequence is in the table, then use the corresponding tag, or any of the tags.\n",
    "* If not, then make a guess (usually: NOUN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(((), 'The', ('quick', 'brown')), {'DET'}),\n",
       "             ((('The',), 'quick', ('brown', 'fox')), {'ADJ'}),\n",
       "             ((('The', 'quick'), 'brown', ('fox', 'jumps')), {'ADJ'}),\n",
       "             ((('quick', 'brown'), 'fox', ('jumps', 'over')), {'NOUN'}),\n",
       "             ((('brown', 'fox'), 'jumps', ('over', 'the')), {'VERB'}),\n",
       "             ((('fox', 'jumps'), 'over', ('the', 'lazy')), {'ADP'}),\n",
       "             ((('jumps', 'over'), 'the', ('lazy', 'dog')), {'DET'}),\n",
       "             ((('over', 'the'), 'lazy', ('dog', '.')), {'ADJ'}),\n",
       "             ((('the', 'lazy'), 'dog', ('.',)), {'NOUN'}),\n",
       "             ((('lazy', 'dog'), '.', ()), {'PUNCT'})])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_lookup2 = OrderedDict()\n",
    "for i in range(len(corpus)):\n",
    "    words_before = tuple(corpus[j][0] for j in range(max(i - 2, 0), i))\n",
    "    word_of_interest = corpus[i][0]\n",
    "    words_after = tuple(corpus[j][0] for j in range(i + 1, min(i + 3, len(corpus))))\n",
    "    if (words_before, word_of_interest, words_after) in pos_lookup2:\n",
    "        pos_lookup2[(words_before, word_of_interest, words_after)] |= {corpus[i][1]}\n",
    "    else:\n",
    "        pos_lookup2[(words_before, word_of_interest, words_after)] = {corpus[i][1]}\n",
    "        \n",
    "pos_lookup2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Problems\n",
    "\n",
    "* **Out-of-vocabulary** (**OOV**) words:\n",
    "    * the word in the test set, but not in the train set\n",
    "    * an important metric in evaluation\n",
    "\n",
    "* **Data sparsity**:\n",
    "    * a word can occur in many contexts, and not all can be in the train set\n",
    "    * inevitable: a language can generate infinitely many sentences\n",
    "    * countermeasures: _back off_ to smaller contexts (or just the target word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example if one takes the word _\"the\"_, then it is most certainly a determiner. However take the following segment:\n",
    "\n",
    "> ... Steve Jobs met the Dalai Lama ...\n",
    "\n",
    "This context might not have occured in the whole history of English (until now!), so it's not in our model. We need to back off:\n",
    "\n",
    "> \"Steve Jobs met\", \"the\", \"Dalai Lama\" $\\rightarrow$<br/>\n",
    "> \"Steve Jobs met\", \"the\", \"\" $\\rightarrow$<br/>\n",
    "> \"Jobs met\", \"the\", \"\" $\\rightarrow$<br/>\n",
    "> \"met\", \"the\", \"\" $\\rightarrow$<br/>\n",
    "> \"\", \"the\", \"\" $\\rightarrow$\n",
    "\n",
    "Luckily in this case, $P(DET|the)\\approx1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hidden Markov Model\n",
    "\n",
    "* Like the Markov model, we take only the $n$ preceding tokens into consideration\n",
    "* The idea behind the model is very different:\n",
    "    * We imagine an automaton that is always in a **(hidden) state**\n",
    "    * In each state, it emits something we can observe\n",
    "    * The task is to find out which is _the most probable_ state sequence that generates the observations\n",
    "* In the POS tagging context,\n",
    "    * The words in the text are the **observed events**\n",
    "    * The POS tags are the hidden states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "That is, we will think of a sentence as the sequence of POS tags, for which the actual choice of words is rather arbitrary.\n",
    "\n",
    "|PRON|VERB |PRON|PREP|NOUN|PUNCT|\n",
    "|---|-----|-----|----|--------|--|\n",
    "| I | saw | you | in | school | .|\n",
    "|You| saw |  me | in | school | .|\n",
    "|You| met |  me | in | school | .|\n",
    "|You| met |  me | in | work | .|\n",
    "|We| met |him | at | work | .|\n",
    "\n",
    "We are presented with one of the sentences and the task is to reconstruct the POS sequence at the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Probabilistic model\n",
    "\n",
    "The HMM is a probabilistic model. Translating \"looking for the tag sequence that best explains the sentence\" to mathematical notation gives us\n",
    "\n",
    "$$\n",
    "{\\arg\\max}_{l_i\\in L}\\mathbb{P}(w_1, w_2, \\ldots w_N \\ | \\ l_1, l_2 \\ldots l_N)\n",
    "$$\n",
    "\n",
    ", i.e.\n",
    "* $\\mathbb{P}(w_1, w_2, \\ldots w_N \\ | \\ l_1, l_2 \\ldots l_N)$ is the probability that a particular $l_1, \\ldots, l_N$ sequence generated the sentence $w_1, \\ldots, w_N$\n",
    "* ${\\arg\\max}_{l_i}$ looks for the $l$ sequence that produces the highest probability.\n",
    "\n",
    "<!-- THIS SHOULD GO TO ML BASICS NEXT YEAR.\n",
    "\n",
    "* We are given a list of probabilities ($N$ is the sentence length)\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(w_1, w_2, \\ldots w_N, l_1, l_2 \\ldots l_N)\n",
    "$$\n",
    "\n",
    "For example,\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(\\text{the}, \\text{dog}, \\text{saw}, \\text{the}, \\text{cat}, \\text{DET}, \\text{NOUN}, \\text{VERB}, \\text{DET}, \\text{NOUN})\n",
    "$$ -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Without any restriction on the probability,\n",
    "* the only way to find the best state sequence is to compute all probabilities\n",
    "* this incurs $\\mathcal{O}(|L|^N)$ complexity\n",
    "* for a sentence of 15 words, this is already around 14M numbers to compute!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In HMM, we make three assumptions to make the computation tractable:\n",
    "1. The current POS tag only depends on a fixed window of $n$ past tokens; $n$ is a parameter of the model (the _Markov assumption_)\n",
    "1. The current word only depends on the current POS tag\n",
    "1. The words are generated independently of one another\n",
    "\n",
    "| I | saw | you | in | school | .|\n",
    "|---|:-----|-----|----:|--------|--|\n",
    "|DET|VERB |PRON|PREP|NOUN|PUNCT|\n",
    "|   |     |    | $w_i$ |||\n",
    "|   |$l_{i-2}$|$l_{i-1}$|$l_i$|||\n",
    "|   | [  |window|  ] ||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With this, the probability can be decomposed as:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(w_1, w_2, \\ldots w_N \\ | \\ l_1, l_2 \\ldots l_N) =\n",
    "    \\prod_{i=1}^N\\mathbb{P}(l_i \\ |\\ l_{i-n+1}, l_{i-n+2}\\ldots l_{i-1})\\cdot\\mathbb{P}(w_i \\ | \\ l_i)\n",
    "$$\n",
    "\n",
    "* The term $\\mathbb{P}(l_i \\ |\\ l_{i-n+1},l_{i-n+2}\\ldots l_{i-1})$ is the **transition probability**: the probability of tag $l_i$, given the previous $n-1$ tags.\n",
    "* The terms $\\mathbb{P}(w_i|l_i)$ are the **emission probabilities**: the probability of a word given its POS tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Training\n",
    "<a id=\"hmm_training\"></a>\n",
    "\n",
    "The transition probabilities are estimated from the n-gram frequencies in the corpus.\n",
    "\n",
    "$$\\mathbb{P}(l_n\\ |\\ l_1,l_2\\ldots l_{n-1})\\approx \\frac{\\#\\{l_1,l_2\\ldots l_{n-1},l_n\\}}{\\#\\{l_1,l_2\\ldots l_{n-1}\\}}$$\n",
    "\n",
    "If the index is not positive (at the beginning of sentences), then we simply omit it and fall back to a lower n-gram order. If normally we are estimating trigrams ($n=3$), we fall back to bi- and unigrams:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\mathbb{P}(l_2\\ |\\ l_0, l_1) & \\approx \\mathbb{P}(l_2\\ |\\ l_1) & = \\frac{\\#\\{l_1,l_2\\}}{\\#\\{l_1\\}} \\quad \\text{and } \\\\\n",
    "\\mathbb{P}(l_1\\ |\\ l_{-1},l_0) & \\approx \\mathbb{P}(l_1) & = \\frac{\\#\\{l_1\\}}{\\text{# of words}}\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Emission probabilities are computed similarly:\n",
    "\n",
    "$$ \\mathbb{P}(w_i|l_i) \\approx \\frac{\\#\\{\\text{word }w_i \\text{ with the tag }l_i\\}}{\\#\\{l_i\\}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The effect of data sparsity on an HMM is much smaller than on an n-gram model:\n",
    "* The size of the lookup table depends on the size of the tagset ($\\mathcal{O}(|L|^n)$, not $\\mathcal{O}(|V|^n)$)\n",
    "* The size of the tagset is way smaller than the vocabulary ($|L| \\ll |V|$)\n",
    "* This results in a much denser table.\n",
    "\n",
    "<!-- For example if you see the word _\"dog\"_ many times in the corpus, and never with the POS tag _DET_ then you can be pretty sure that it will never have that tag. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Inference\n",
    "\n",
    "* At prediction (inference) time, given a sequence of words, we need to find the ${\\arg\\max}$.\n",
    "* The naive algorithm would just compute the probabilities for all $l_1, \\ldots, l_N$, incurring $\\mathcal{O}(|L|^N)$ complexity.\n",
    "* Because of how we decomposed the probability, we can use a more efficient algorithm.\n",
    "\n",
    "**Dynamic Programming (DP)** algorithms solve complex problems by breaking it up into smaller sub-problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Viterbi algorithm\n",
    "\n",
    "The **Viterbi algorithm** has two phases:\n",
    "* **Forward phase**: goes through the sentence from beginning to end and fills the probability and backpointer tables;\n",
    "* **Backward phase**: follows the backpointers to find the most probable state sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We detail the case of bigrams, $n=2$. The input of the algorithm:\n",
    "* the sentence $W = \\{w_1, \\ldots,w_N\\}$\n",
    "* the hidden state space $S=\\{s_1, \\ldots , s_{|L|}\\}$ (the POS tags)\n",
    "* the transition probabilities $T_{i,j} = \\mathbb{P}(l_t=s_j|l_{t-1}=s_i)$\n",
    "* the emission probabilities $E_{i,j} = \\mathbb{P}(w_j|l_t=s_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The algorithm outputs two ($|L| \\times N$) tables:\n",
    "* $\\pi[i,j]$: the probability of the most likely hidden state sequence that ends with $l_j=s_i$\n",
    "* $B[i,j]$: the backpointers along the most probable transitions\n",
    "\n",
    "Since the algorithm only fills these two tables, the complexity is only $\\mathcal{O}(N \\cdot |L|^n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Example\n",
    "\n",
    "We demonstrate the algorithm on the sentence\n",
    "\n",
    "|DET|NOUN|VERB|DET|NOUN|\n",
    "|---|---|---|-|---|\n",
    "|the|dog|saw|a|cat|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The tables $\\pi$ and $B$ then look like (the stars show the state sequence we are looking for):\n",
    "\n",
    "| |the|dog|saw|a|cat|\n",
    "|-|---|---|---|-|---|\n",
    "|**DET**| *|   |   | *|   |\n",
    "|**NOUN**||   *|   | |   *|\n",
    "|**VERB**||   |   *| |   |\n",
    "\n",
    "The tables are filled left-to-right, column-by-column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$\\pi$ implements the computation:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(w_1, w_2, \\ldots w_N \\ | \\ l_1, l_2 \\ldots l_N) =\n",
    "    \\prod_{t=1}^N\\mathbb{P}(l_t \\ |\\ l_{t-1})\\cdot\\mathbb{P}(w_t \\ | \\ l_t)\n",
    "$$\n",
    "\n",
    "Note this is the same as before, but\n",
    "* $n = 2$, so the transition probability is simpler\n",
    "* the running index to $t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In particular, the cell $\\pi[i,j]$ records the probability of\n",
    "* the most likely state sequence $l_1, \\ldots, l_j$\n",
    "* that produced the words $w_1, \\ldots, w_j$. \n",
    "\n",
    "To compute that probability, we need\n",
    "* the probability of the path thus far. Since we don't know the previous state, we need to consider all cells in the previous column $\\pi[*,j-1]$\n",
    "* the emission probability for the current cell $E_{i,j}$\n",
    "* the transition probabilities between the current cell and a cell in the previous column $T_{*,i}$\n",
    "* finally, we take the maximum of the above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Putting that to equations,\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\pi[i,j] & = \\max_{l_1, \\ldots, l_j}\\mathbb{P}(w_1, \\ldots, w_j \\ |\\ l_1, \\ldots, l_j) \\\\\n",
    "         & = \\max_{l_t, \\forall t}\\prod_{t=1}^j\\mathbb{P}(l_t \\ |\\ l_{t-1})\\cdot\\mathbb{P}(w_t \\ | \\ l_t) \\\\\n",
    "         & = \\max_{l_t, \\forall t} \\prod_{t=1}^{j-1}\\mathbb{P}(l_t \\ |\\ l_{t-1})\\cdot\\mathbb{P}(w_t \\ | \\ l_t)\n",
    "             & \\max_k \\mathbb{P}(l_j=s_i | l_{j-1}=s_k) & \\cdot \\mathbb{P}(w_j | l_j=s_i) \\\\\n",
    "         & = \\max_k \\pi[k,j-1] & \\cdot \\mathbb{P}(l_j=s_i | l_{j-1}=s_k) & \\cdot \\mathbb{P}(w_j | l_j=s_i) \\\\\n",
    "         & = \\max_k \\pi[k,j-1] & \\cdot T_{k,j} & \\cdot E_{i,j}\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the end, the maximum value in the last column $\\max_{i \\in |L|}\\pi[i,N]$ is the probability of the most likely tag sequence.\n",
    "* This tells us the most probable state (POS tag) to end the sequence.\n",
    "* But what about the rest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<a id=\"aha\">This</a> is where table $B$ enters the picture:\n",
    "* when computing $\\pi[i,j]$, we note which state transition $k \\rightarrow i$ resulted in the highest probability\n",
    "* we store that information in $B[i,j]$ as **backpointer** to $B[k, j-1]$.\n",
    "\n",
    "In the backward phase, all we have to do is follow the backpointers from $B[i,N]$ ($i$ is where $\\pi[i,N]$ takes its maximum) to find all most probable state sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The algorithm in pseudocode:\n",
    "\n",
    "<!-- set $\\pi[i,0] = 1, \\forall i \\in 1 \\ldots |L|$<br/>-->\n",
    "> begin for $j = 1 \\ldots N$<br/>\n",
    ">   $\\quad$   begin for $i = 1 \\ldots |L|$<br/>\n",
    ">   $\\quad \\quad$  $\\pi[i,j] = \\max_{k \\in 1:|L|} \\pi[k,j-1]\\cdot T_{k,i} \\cdot E_{i,j}$<br/>\n",
    ">   $\\quad \\quad$  $B[i,j]={\\arg\\max}_k \\pi[k,j-1] \\cdot T_{k,i}$<br/>\n",
    ">   $\\quad$ end for<br/>\n",
    "> end for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Notes about the algorithm\n",
    "\n",
    "1. When computing $T_{i,1}$, there is no state to transition from. Use the unigram probabilities in this case.\n",
    "1. Note that the uni- and bigrams collected in the [HMM Training slide](#hmm_training) are not the general uni- or bigram distributions, as we only collect $n-1$, $n-2$, etc.-grams at the beginning of sentences. This is the reason why you can use the unigram distribution for $T_{i,1}$ $-$ and also why you should use them **only** there.\n",
    "1. The pages above described the Viterbi algorithm for bigram ($n=2$) transition probabilities. When going to $n=3$ (and beyond):\n",
    "    * the transition probabilities have the form $\\mathbb{P}(l_t\\ |\\ l_{t-1},l_{t-2})$\n",
    "    * at the beginning of the sentence, use the unigram distribution for the first word and the bigram for the second\n",
    "    * consequently, the tables must have $n$ dimensions: $|L|\\times|L|\\times N$ for $n=3$, etc.\n",
    "    * let's say that the second dimension corresponds to the current tag, the first to the previous one\n",
    "    * the update rule then changes as: $\\pi[i,j,k] = \\max_l \\pi[l,j,k-1] \\cdot T_{l,i,j} \\cdot E_{i,j}$\n",
    "1. See more: https://courses.engr.illinois.edu/cs447/fa2017/Slides/Lecture07.pdf"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
